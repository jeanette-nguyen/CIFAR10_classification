Building CIFAR-10 data loader with 1 workers
Files already downloaded and verified
CIFAR-10 training data size: 50000
Files already downloaded and verified
CIFAR-10 testing data size: 10000
=> creating model 'resnet20'
Epoch: [0/80][0/250]	LR: 0.1	Time 0.491 (0.491)	Data 0.116 (0.116)	Loss 2.5402 (2.5402)	Prec@1 13.500 (13.500)
Epoch: [0/80][100/250]	LR: 0.1	Time 0.036 (0.045)	Data 0.019 (0.024)	Loss 1.7128 (1.8513)	Prec@1 37.000 (29.574)
Epoch: [0/80][200/250]	LR: 0.1	Time 0.045 (0.042)	Data 0.024 (0.024)	Loss 1.4217 (1.6685)	Prec@1 48.000 (37.209)
 * Training Prec@1 40.314
main.py:153: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  losses.update(loss.data[0], input.size(0))
Test: [0/50]	Time 0.148 (0.148)	Loss 1.5975 (1.5975)	Prec@1 48.000 (48.000)
 * Testing Prec@1 48.040
main.py:190: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  input_var = torch.autograd.Variable(input, volatile=True)
main.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  target_var = torch.autograd.Variable(target, volatile=True)
main.py:199: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  losses.update(loss.data[0], input.size(0))
Epoch: [1/80][0/250]	LR: 0.1	Time 0.122 (0.122)	Data 0.086 (0.086)	Loss 1.2267 (1.2267)	Prec@1 54.500 (54.500)
Epoch: [1/80][100/250]	LR: 0.1	Time 0.040 (0.042)	Data 0.026 (0.026)	Loss 1.1672 (1.1701)	Prec@1 59.000 (57.723)
Epoch: [1/80][200/250]	LR: 0.1	Time 0.049 (0.042)	Data 0.029 (0.026)	Loss 1.1576 (1.1047)	Prec@1 59.000 (60.072)
 * Training Prec@1 61.018
Test: [0/50]	Time 0.164 (0.164)	Loss 1.4170 (1.4170)	Prec@1 55.000 (55.000)
 * Testing Prec@1 53.110
Epoch: [2/80][0/250]	LR: 0.1	Time 0.114 (0.114)	Data 0.091 (0.091)	Loss 0.9443 (0.9443)	Prec@1 63.000 (63.000)
Epoch: [2/80][100/250]	LR: 0.1	Time 0.043 (0.043)	Data 0.024 (0.027)	Loss 0.8558 (0.9150)	Prec@1 71.500 (67.411)
Epoch: [2/80][200/250]	LR: 0.1	Time 0.045 (0.043)	Data 0.027 (0.027)	Loss 0.8866 (0.8830)	Prec@1 67.500 (68.585)
 * Training Prec@1 69.308
Test: [0/50]	Time 0.116 (0.116)	Loss 0.9822 (0.9822)	Prec@1 64.500 (64.500)
 * Testing Prec@1 65.180
Epoch: [3/80][0/250]	LR: 0.1	Time 0.104 (0.104)	Data 0.074 (0.074)	Loss 0.7046 (0.7046)	Prec@1 74.500 (74.500)
Epoch: [3/80][100/250]	LR: 0.1	Time 0.042 (0.041)	Data 0.021 (0.025)	Loss 0.7258 (0.7656)	Prec@1 74.500 (73.045)
Epoch: [3/80][200/250]	LR: 0.1	Time 0.043 (0.043)	Data 0.028 (0.027)	Loss 0.7270 (0.7436)	Prec@1 75.000 (73.935)
 * Training Prec@1 74.206
Test: [0/50]	Time 0.153 (0.153)	Loss 0.9318 (0.9318)	Prec@1 71.000 (71.000)
 * Testing Prec@1 69.090
Epoch: [4/80][0/250]	LR: 0.1	Time 0.116 (0.116)	Data 0.086 (0.086)	Loss 0.7028 (0.7028)	Prec@1 76.000 (76.000)
Epoch: [4/80][100/250]	LR: 0.1	Time 0.041 (0.045)	Data 0.025 (0.029)	Loss 0.7521 (0.6831)	Prec@1 74.000 (76.109)
Epoch: [4/80][200/250]	LR: 0.1	Time 0.050 (0.044)	Data 0.031 (0.028)	Loss 0.6429 (0.6604)	Prec@1 77.000 (77.144)
 * Training Prec@1 77.352
Test: [0/50]	Time 0.126 (0.126)	Loss 0.7808 (0.7808)	Prec@1 76.500 (76.500)
 * Testing Prec@1 74.270
Epoch: [5/80][0/250]	LR: 0.1	Time 0.138 (0.138)	Data 0.109 (0.109)	Loss 0.6193 (0.6193)	Prec@1 78.500 (78.500)
Epoch: [5/80][100/250]	LR: 0.1	Time 0.049 (0.046)	Data 0.031 (0.029)	Loss 0.6565 (0.6117)	Prec@1 73.500 (78.574)
Epoch: [5/80][200/250]	LR: 0.1	Time 0.038 (0.045)	Data 0.022 (0.028)	Loss 0.5843 (0.6121)	Prec@1 80.000 (78.682)
 * Training Prec@1 78.756
Test: [0/50]	Time 0.131 (0.131)	Loss 0.5782 (0.5782)	Prec@1 80.000 (80.000)
 * Testing Prec@1 77.900
Epoch: [6/80][0/250]	LR: 0.1	Time 0.126 (0.126)	Data 0.092 (0.092)	Loss 0.6086 (0.6086)	Prec@1 78.000 (78.000)
Epoch: [6/80][100/250]	LR: 0.1	Time 0.047 (0.048)	Data 0.027 (0.031)	Loss 0.5266 (0.5646)	Prec@1 80.000 (80.337)
Epoch: [6/80][200/250]	LR: 0.1	Time 0.052 (0.047)	Data 0.034 (0.031)	Loss 0.6137 (0.5656)	Prec@1 78.500 (80.254)
 * Training Prec@1 80.392
Test: [0/50]	Time 0.100 (0.100)	Loss 0.7287 (0.7287)	Prec@1 75.500 (75.500)
 * Testing Prec@1 75.620
Epoch: [7/80][0/250]	LR: 0.1	Time 0.113 (0.113)	Data 0.084 (0.084)	Loss 0.5029 (0.5029)	Prec@1 82.000 (82.000)
Epoch: [7/80][100/250]	LR: 0.1	Time 0.040 (0.046)	Data 0.022 (0.028)	Loss 0.5238 (0.5197)	Prec@1 80.500 (81.787)
Epoch: [7/80][200/250]	LR: 0.1	Time 0.053 (0.045)	Data 0.033 (0.028)	Loss 0.4299 (0.5243)	Prec@1 85.500 (81.711)
 * Training Prec@1 81.740
Test: [0/50]	Time 0.108 (0.108)	Loss 0.6820 (0.6820)	Prec@1 77.500 (77.500)
 * Testing Prec@1 76.880
Epoch: [8/80][0/250]	LR: 0.1	Time 0.124 (0.124)	Data 0.096 (0.096)	Loss 0.3887 (0.3887)	Prec@1 87.500 (87.500)
Epoch: [8/80][100/250]	LR: 0.1	Time 0.037 (0.044)	Data 0.022 (0.026)	Loss 0.4411 (0.4942)	Prec@1 83.500 (82.891)
Epoch: [8/80][200/250]	LR: 0.1	Time 0.040 (0.044)	Data 0.021 (0.026)	Loss 0.4723 (0.4985)	Prec@1 84.500 (82.679)
 * Training Prec@1 82.712
Test: [0/50]	Time 0.154 (0.154)	Loss 0.7703 (0.7703)	Prec@1 73.500 (73.500)
 * Testing Prec@1 77.580
Epoch: [9/80][0/250]	LR: 0.1	Time 0.144 (0.144)	Data 0.114 (0.114)	Loss 0.5029 (0.5029)	Prec@1 80.500 (80.500)
Epoch: [9/80][100/250]	LR: 0.1	Time 0.035 (0.044)	Data 0.021 (0.027)	Loss 0.4092 (0.4765)	Prec@1 86.000 (83.371)
Epoch: [9/80][200/250]	LR: 0.1	Time 0.043 (0.042)	Data 0.024 (0.026)	Loss 0.5638 (0.4784)	Prec@1 79.000 (83.269)
 * Training Prec@1 83.304
Test: [0/50]	Time 0.223 (0.223)	Loss 0.6390 (0.6390)	Prec@1 78.500 (78.500)
 * Testing Prec@1 80.180
Epoch: [10/80][0/250]	LR: 0.1	Time 0.133 (0.133)	Data 0.103 (0.103)	Loss 0.4620 (0.4620)	Prec@1 82.500 (82.500)
Epoch: [10/80][100/250]	LR: 0.1	Time 0.057 (0.044)	Data 0.043 (0.028)	Loss 0.4122 (0.4406)	Prec@1 86.000 (84.653)
Epoch: [10/80][200/250]	LR: 0.1	Time 0.045 (0.044)	Data 0.025 (0.028)	Loss 0.4727 (0.4518)	Prec@1 83.500 (84.470)
 * Training Prec@1 84.380
Test: [0/50]	Time 0.128 (0.128)	Loss 0.7213 (0.7213)	Prec@1 74.000 (74.000)
 * Testing Prec@1 77.800
Epoch: [11/80][0/250]	LR: 0.1	Time 0.139 (0.139)	Data 0.115 (0.115)	Loss 0.4168 (0.4168)	Prec@1 84.000 (84.000)
Epoch: [11/80][100/250]	LR: 0.1	Time 0.050 (0.046)	Data 0.032 (0.030)	Loss 0.5147 (0.4336)	Prec@1 81.500 (85.208)
Epoch: [11/80][200/250]	LR: 0.1	Time 0.044 (0.045)	Data 0.023 (0.029)	Loss 0.4943 (0.4404)	Prec@1 82.000 (84.831)
 * Training Prec@1 84.920
Test: [0/50]	Time 0.111 (0.111)	Loss 0.6679 (0.6679)	Prec@1 78.500 (78.500)
 * Testing Prec@1 79.730
Epoch: [12/80][0/250]	LR: 0.1	Time 0.147 (0.147)	Data 0.121 (0.121)	Loss 0.2970 (0.2970)	Prec@1 90.000 (90.000)
Epoch: [12/80][100/250]	LR: 0.1	Time 0.034 (0.042)	Data 0.019 (0.025)	Loss 0.3757 (0.4196)	Prec@1 86.500 (85.480)
Epoch: [12/80][200/250]	LR: 0.1	Time 0.071 (0.042)	Data 0.053 (0.026)	Loss 0.3268 (0.4193)	Prec@1 87.500 (85.383)
 * Training Prec@1 85.444
Test: [0/50]	Time 0.157 (0.157)	Loss 0.5470 (0.5470)	Prec@1 80.500 (80.500)
 * Testing Prec@1 82.430
Epoch: [13/80][0/250]	LR: 0.1	Time 0.153 (0.153)	Data 0.120 (0.120)	Loss 0.5127 (0.5127)	Prec@1 81.500 (81.500)
Epoch: [13/80][100/250]	LR: 0.1	Time 0.050 (0.044)	Data 0.031 (0.027)	Loss 0.4212 (0.4075)	Prec@1 87.500 (86.059)
Epoch: [13/80][200/250]	LR: 0.1	Time 0.036 (0.044)	Data 0.022 (0.028)	Loss 0.2621 (0.4048)	Prec@1 91.500 (86.109)
 * Training Prec@1 85.964
Test: [0/50]	Time 0.139 (0.139)	Loss 0.5153 (0.5153)	Prec@1 82.000 (82.000)
 * Testing Prec@1 82.360
Epoch: [14/80][0/250]	LR: 0.1	Time 0.182 (0.182)	Data 0.152 (0.152)	Loss 0.5105 (0.5105)	Prec@1 82.000 (82.000)
Epoch: [14/80][100/250]	LR: 0.1	Time 0.040 (0.043)	Data 0.027 (0.026)	Loss 0.2826 (0.3877)	Prec@1 88.500 (86.460)
Epoch: [14/80][200/250]	LR: 0.1	Time 0.047 (0.042)	Data 0.030 (0.025)	Loss 0.3285 (0.3878)	Prec@1 87.500 (86.577)
 * Training Prec@1 86.600
Test: [0/50]	Time 0.151 (0.151)	Loss 0.5292 (0.5292)	Prec@1 84.000 (84.000)
 * Testing Prec@1 81.240
Epoch: [15/80][0/250]	LR: 0.1	Time 0.159 (0.159)	Data 0.126 (0.126)	Loss 0.3916 (0.3916)	Prec@1 87.000 (87.000)
Epoch: [15/80][100/250]	LR: 0.1	Time 0.041 (0.044)	Data 0.022 (0.027)	Loss 0.4254 (0.3753)	Prec@1 86.500 (87.050)
Epoch: [15/80][200/250]	LR: 0.1	Time 0.049 (0.044)	Data 0.031 (0.026)	Loss 0.4064 (0.3840)	Prec@1 83.000 (86.729)
 * Training Prec@1 86.716
Test: [0/50]	Time 0.114 (0.114)	Loss 0.6392 (0.6392)	Prec@1 75.000 (75.000)
 * Testing Prec@1 79.600
Epoch: [16/80][0/250]	LR: 0.1	Time 0.123 (0.123)	Data 0.092 (0.092)	Loss 0.3245 (0.3245)	Prec@1 88.000 (88.000)
Epoch: [16/80][100/250]	LR: 0.1	Time 0.048 (0.040)	Data 0.030 (0.024)	Loss 0.3591 (0.3598)	Prec@1 89.500 (87.738)
Epoch: [16/80][200/250]	LR: 0.1	Time 0.040 (0.042)	Data 0.024 (0.026)	Loss 0.3152 (0.3668)	Prec@1 88.500 (87.323)
 * Training Prec@1 87.240
Test: [0/50]	Time 0.101 (0.101)	Loss 0.5296 (0.5296)	Prec@1 83.500 (83.500)
 * Testing Prec@1 82.190
Epoch: [17/80][0/250]	LR: 0.1	Time 0.111 (0.111)	Data 0.082 (0.082)	Loss 0.2724 (0.2724)	Prec@1 92.000 (92.000)
Epoch: [17/80][100/250]	LR: 0.1	Time 0.061 (0.044)	Data 0.040 (0.025)	Loss 0.3376 (0.3622)	Prec@1 87.000 (87.678)
Epoch: [17/80][200/250]	LR: 0.1	Time 0.038 (0.044)	Data 0.022 (0.025)	Loss 0.4269 (0.3627)	Prec@1 86.000 (87.505)
 * Training Prec@1 87.500
Test: [0/50]	Time 0.121 (0.121)	Loss 0.5679 (0.5679)	Prec@1 81.500 (81.500)
 * Testing Prec@1 82.390
Epoch: [18/80][0/250]	LR: 0.1	Time 0.109 (0.109)	Data 0.084 (0.084)	Loss 0.4855 (0.4855)	Prec@1 83.500 (83.500)
Epoch: [18/80][100/250]	LR: 0.1	Time 0.058 (0.045)	Data 0.038 (0.028)	Loss 0.3267 (0.3492)	Prec@1 89.000 (87.797)
Epoch: [18/80][200/250]	LR: 0.1	Time 0.046 (0.043)	Data 0.028 (0.025)	Loss 0.3239 (0.3517)	Prec@1 89.000 (87.689)
 * Training Prec@1 87.662
Test: [0/50]	Time 0.121 (0.121)	Loss 0.4781 (0.4781)	Prec@1 84.500 (84.500)
 * Testing Prec@1 84.680
Epoch: [19/80][0/250]	LR: 0.1	Time 0.148 (0.148)	Data 0.124 (0.124)	Loss 0.3569 (0.3569)	Prec@1 87.000 (87.000)
Epoch: [19/80][100/250]	LR: 0.1	Time 0.034 (0.043)	Data 0.014 (0.024)	Loss 0.4151 (0.3403)	Prec@1 83.000 (88.178)
Epoch: [19/80][200/250]	LR: 0.1	Time 0.044 (0.043)	Data 0.024 (0.024)	Loss 0.3156 (0.3388)	Prec@1 90.000 (88.251)
 * Training Prec@1 88.146
Test: [0/50]	Time 0.116 (0.116)	Loss 0.4820 (0.4820)	Prec@1 85.000 (85.000)
 * Testing Prec@1 84.250
Epoch: [20/80][0/250]	LR: 0.1	Time 0.114 (0.114)	Data 0.088 (0.088)	Loss 0.3397 (0.3397)	Prec@1 86.500 (86.500)
Epoch: [20/80][100/250]	LR: 0.1	Time 0.036 (0.042)	Data 0.023 (0.025)	Loss 0.3861 (0.3313)	Prec@1 85.500 (88.322)
Epoch: [20/80][200/250]	LR: 0.1	Time 0.036 (0.042)	Data 0.023 (0.025)	Loss 0.3932 (0.3327)	Prec@1 87.500 (88.445)
 * Training Prec@1 88.310
Test: [0/50]	Time 0.093 (0.093)	Loss 0.4746 (0.4746)	Prec@1 86.000 (86.000)
 * Testing Prec@1 83.980
Epoch: [21/80][0/250]	LR: 0.1	Time 0.113 (0.113)	Data 0.081 (0.081)	Loss 0.3490 (0.3490)	Prec@1 86.000 (86.000)
Epoch: [21/80][100/250]	LR: 0.1	Time 0.049 (0.046)	Data 0.027 (0.027)	Loss 0.3513 (0.3304)	Prec@1 86.000 (88.431)
Epoch: [21/80][200/250]	LR: 0.1	Time 0.043 (0.045)	Data 0.027 (0.027)	Loss 0.2650 (0.3348)	Prec@1 91.000 (88.187)
 * Training Prec@1 88.278
Test: [0/50]	Time 0.087 (0.087)	Loss 0.5543 (0.5543)	Prec@1 82.500 (82.500)
 * Testing Prec@1 84.850
Epoch: [22/80][0/250]	LR: 0.1	Time 0.114 (0.114)	Data 0.084 (0.084)	Loss 0.3043 (0.3043)	Prec@1 88.500 (88.500)
Epoch: [22/80][100/250]	LR: 0.1	Time 0.045 (0.043)	Data 0.025 (0.026)	Loss 0.2546 (0.3154)	Prec@1 94.000 (89.064)
Epoch: [22/80][200/250]	LR: 0.1	Time 0.035 (0.043)	Data 0.017 (0.026)	Loss 0.2359 (0.3176)	Prec@1 93.000 (88.925)
 * Training Prec@1 88.908
Test: [0/50]	Time 0.138 (0.138)	Loss 0.5231 (0.5231)	Prec@1 84.500 (84.500)
 * Testing Prec@1 83.820
Epoch: [23/80][0/250]	LR: 0.1	Time 0.156 (0.156)	Data 0.129 (0.129)	Loss 0.2261 (0.2261)	Prec@1 93.500 (93.500)
Epoch: [23/80][100/250]	LR: 0.1	Time 0.041 (0.043)	Data 0.025 (0.025)	Loss 0.1918 (0.3031)	Prec@1 93.000 (89.257)
Epoch: [23/80][200/250]	LR: 0.1	Time 0.040 (0.042)	Data 0.025 (0.024)	Loss 0.4437 (0.3123)	Prec@1 84.500 (89.090)
 * Training Prec@1 88.968
Test: [0/50]	Time 0.107 (0.107)	Loss 0.5759 (0.5759)	Prec@1 82.000 (82.000)
 * Testing Prec@1 83.200
Epoch: [24/80][0/250]	LR: 0.1	Time 0.181 (0.181)	Data 0.155 (0.155)	Loss 0.2761 (0.2761)	Prec@1 89.500 (89.500)
Epoch: [24/80][100/250]	LR: 0.1	Time 0.035 (0.045)	Data 0.023 (0.027)	Loss 0.3769 (0.3093)	Prec@1 85.500 (89.104)
Epoch: [24/80][200/250]	LR: 0.1	Time 0.041 (0.043)	Data 0.025 (0.025)	Loss 0.2820 (0.3134)	Prec@1 89.000 (88.893)
 * Training Prec@1 88.816
Test: [0/50]	Time 0.104 (0.104)	Loss 0.5070 (0.5070)	Prec@1 86.500 (86.500)
 * Testing Prec@1 81.540
Epoch: [25/80][0/250]	LR: 0.1	Time 0.153 (0.153)	Data 0.121 (0.121)	Loss 0.2279 (0.2279)	Prec@1 91.000 (91.000)
Epoch: [25/80][100/250]	LR: 0.1	Time 0.035 (0.043)	Data 0.018 (0.025)	Loss 0.3317 (0.2986)	Prec@1 90.000 (89.510)
Epoch: [25/80][200/250]	LR: 0.1	Time 0.053 (0.043)	Data 0.039 (0.026)	Loss 0.3286 (0.3048)	Prec@1 88.500 (89.368)
 * Training Prec@1 89.338
Test: [0/50]	Time 0.100 (0.100)	Loss 0.5862 (0.5862)	Prec@1 82.500 (82.500)
 * Testing Prec@1 83.670
Epoch: [26/80][0/250]	LR: 0.1	Time 0.144 (0.144)	Data 0.112 (0.112)	Loss 0.2689 (0.2689)	Prec@1 91.500 (91.500)
Epoch: [26/80][100/250]	LR: 0.1	Time 0.035 (0.042)	Data 0.018 (0.025)	Loss 0.3735 (0.2996)	Prec@1 88.500 (89.653)
Epoch: [26/80][200/250]	LR: 0.1	Time 0.046 (0.041)	Data 0.029 (0.024)	Loss 0.3560 (0.3052)	Prec@1 88.000 (89.296)
 * Training Prec@1 89.214
Test: [0/50]	Time 0.132 (0.132)	Loss 0.5129 (0.5129)	Prec@1 85.000 (85.000)
 * Testing Prec@1 86.010
Epoch: [27/80][0/250]	LR: 0.1	Time 0.159 (0.159)	Data 0.118 (0.118)	Loss 0.2991 (0.2991)	Prec@1 88.000 (88.000)
Epoch: [27/80][100/250]	LR: 0.1	Time 0.037 (0.041)	Data 0.020 (0.025)	Loss 0.2988 (0.2827)	Prec@1 88.000 (89.941)
Epoch: [27/80][200/250]	LR: 0.1	Time 0.039 (0.040)	Data 0.025 (0.024)	Loss 0.3784 (0.2919)	Prec@1 87.000 (89.662)
 * Training Prec@1 89.470
Test: [0/50]	Time 0.162 (0.162)	Loss 0.5560 (0.5560)	Prec@1 82.000 (82.000)
 * Testing Prec@1 81.980
Epoch: [28/80][0/250]	LR: 0.1	Time 0.127 (0.127)	Data 0.100 (0.100)	Loss 0.2862 (0.2862)	Prec@1 87.500 (87.500)
Epoch: [28/80][100/250]	LR: 0.1	Time 0.055 (0.043)	Data 0.035 (0.026)	Loss 0.2952 (0.2792)	Prec@1 91.000 (90.302)
Epoch: [28/80][200/250]	LR: 0.1	Time 0.032 (0.041)	Data 0.014 (0.024)	Loss 0.2514 (0.2891)	Prec@1 90.500 (89.888)
 * Training Prec@1 89.758
Test: [0/50]	Time 0.144 (0.144)	Loss 0.8013 (0.8013)	Prec@1 76.500 (76.500)
 * Testing Prec@1 79.750
Epoch: [29/80][0/250]	LR: 0.1	Time 0.122 (0.122)	Data 0.089 (0.089)	Loss 0.2346 (0.2346)	Prec@1 91.000 (91.000)
Epoch: [29/80][100/250]	LR: 0.1	Time 0.043 (0.040)	Data 0.024 (0.023)	Loss 0.2842 (0.2775)	Prec@1 89.000 (90.302)
Epoch: [29/80][200/250]	LR: 0.1	Time 0.034 (0.041)	Data 0.019 (0.024)	Loss 0.2905 (0.2861)	Prec@1 92.000 (89.988)
 * Training Prec@1 89.878
Test: [0/50]	Time 0.133 (0.133)	Loss 0.7608 (0.7608)	Prec@1 79.000 (79.000)
 * Testing Prec@1 81.340
Epoch: [30/80][0/250]	LR: 0.1	Time 0.115 (0.115)	Data 0.085 (0.085)	Loss 0.2260 (0.2260)	Prec@1 90.500 (90.500)
Epoch: [30/80][100/250]	LR: 0.1	Time 0.069 (0.044)	Data 0.044 (0.027)	Loss 0.2282 (0.2670)	Prec@1 93.000 (90.762)
Epoch: [30/80][200/250]	LR: 0.1	Time 0.040 (0.044)	Data 0.024 (0.026)	Loss 0.3871 (0.2756)	Prec@1 89.000 (90.333)
 * Training Prec@1 90.288
Test: [0/50]	Time 0.152 (0.152)	Loss 0.5638 (0.5638)	Prec@1 83.500 (83.500)
 * Testing Prec@1 85.420
Epoch: [31/80][0/250]	LR: 0.1	Time 0.193 (0.193)	Data 0.166 (0.166)	Loss 0.2647 (0.2647)	Prec@1 90.000 (90.000)
Epoch: [31/80][100/250]	LR: 0.1	Time 0.038 (0.044)	Data 0.021 (0.027)	Loss 0.1944 (0.2702)	Prec@1 93.500 (90.574)
Epoch: [31/80][200/250]	LR: 0.1	Time 0.036 (0.041)	Data 0.020 (0.024)	Loss 0.2691 (0.2740)	Prec@1 90.500 (90.400)
 * Training Prec@1 90.326
Test: [0/50]	Time 0.123 (0.123)	Loss 0.5262 (0.5262)	Prec@1 82.000 (82.000)
 * Testing Prec@1 85.260
Epoch: [32/80][0/250]	LR: 0.1	Time 0.192 (0.192)	Data 0.157 (0.157)	Loss 0.2832 (0.2832)	Prec@1 90.000 (90.000)
Epoch: [32/80][100/250]	LR: 0.1	Time 0.038 (0.042)	Data 0.023 (0.025)	Loss 0.2171 (0.2640)	Prec@1 93.000 (90.678)
Epoch: [32/80][200/250]	LR: 0.1	Time 0.043 (0.041)	Data 0.024 (0.023)	Loss 0.2848 (0.2687)	Prec@1 89.500 (90.560)
 * Training Prec@1 90.484
Test: [0/50]	Time 0.146 (0.146)	Loss 0.5054 (0.5054)	Prec@1 81.500 (81.500)
 * Testing Prec@1 84.930
Epoch: [33/80][0/250]	LR: 0.1	Time 0.176 (0.176)	Data 0.150 (0.150)	Loss 0.2625 (0.2625)	Prec@1 90.000 (90.000)
Epoch: [33/80][100/250]	LR: 0.1	Time 0.057 (0.041)	Data 0.037 (0.025)	Loss 0.2628 (0.2642)	Prec@1 89.000 (90.619)
Epoch: [33/80][200/250]	LR: 0.1	Time 0.038 (0.040)	Data 0.021 (0.024)	Loss 0.2733 (0.2662)	Prec@1 92.000 (90.577)
 * Training Prec@1 90.440
Test: [0/50]	Time 0.166 (0.166)	Loss 0.4242 (0.4242)	Prec@1 86.500 (86.500)
 * Testing Prec@1 85.820
Epoch: [34/80][0/250]	LR: 0.1	Time 0.152 (0.152)	Data 0.122 (0.122)	Loss 0.2881 (0.2881)	Prec@1 90.500 (90.500)
Epoch: [34/80][100/250]	LR: 0.1	Time 0.034 (0.043)	Data 0.019 (0.026)	Loss 0.3714 (0.2628)	Prec@1 89.500 (90.757)
Epoch: [34/80][200/250]	LR: 0.1	Time 0.040 (0.041)	Data 0.020 (0.024)	Loss 0.2631 (0.2655)	Prec@1 90.500 (90.694)
 * Training Prec@1 90.534
Test: [0/50]	Time 0.143 (0.143)	Loss 0.4594 (0.4594)	Prec@1 84.000 (84.000)
 * Testing Prec@1 84.450
Epoch: [35/80][0/250]	LR: 0.1	Time 0.127 (0.127)	Data 0.089 (0.089)	Loss 0.3168 (0.3168)	Prec@1 88.500 (88.500)
Epoch: [35/80][100/250]	LR: 0.1	Time 0.044 (0.041)	Data 0.026 (0.023)	Loss 0.2841 (0.2523)	Prec@1 89.000 (91.337)
Epoch: [35/80][200/250]	LR: 0.1	Time 0.037 (0.041)	Data 0.024 (0.024)	Loss 0.2493 (0.2630)	Prec@1 90.000 (90.801)
 * Training Prec@1 90.816
Test: [0/50]	Time 0.174 (0.174)	Loss 0.5296 (0.5296)	Prec@1 84.500 (84.500)
 * Testing Prec@1 82.530
Epoch: [36/80][0/250]	LR: 0.1	Time 0.104 (0.104)	Data 0.074 (0.074)	Loss 0.2641 (0.2641)	Prec@1 90.500 (90.500)
Epoch: [36/80][100/250]	LR: 0.1	Time 0.039 (0.043)	Data 0.023 (0.025)	Loss 0.2482 (0.2468)	Prec@1 93.000 (91.342)
Epoch: [36/80][200/250]	LR: 0.1	Time 0.044 (0.042)	Data 0.023 (0.025)	Loss 0.2702 (0.2543)	Prec@1 89.000 (91.060)
 * Training Prec@1 90.918
Test: [0/50]	Time 0.117 (0.117)	Loss 0.5262 (0.5262)	Prec@1 82.000 (82.000)
 * Testing Prec@1 85.530
Epoch: [37/80][0/250]	LR: 0.1	Time 0.108 (0.108)	Data 0.076 (0.076)	Loss 0.1985 (0.1985)	Prec@1 92.000 (92.000)
Epoch: [37/80][100/250]	LR: 0.1	Time 0.057 (0.043)	Data 0.041 (0.025)	Loss 0.2842 (0.2491)	Prec@1 88.500 (91.079)
Epoch: [37/80][200/250]	LR: 0.1	Time 0.038 (0.041)	Data 0.024 (0.024)	Loss 0.1996 (0.2591)	Prec@1 93.500 (90.831)
 * Training Prec@1 90.838
Test: [0/50]	Time 0.103 (0.103)	Loss 0.5597 (0.5597)	Prec@1 84.000 (84.000)
 * Testing Prec@1 85.830
Epoch: [38/80][0/250]	LR: 0.1	Time 0.113 (0.113)	Data 0.075 (0.075)	Loss 0.1347 (0.1347)	Prec@1 96.500 (96.500)
Epoch: [38/80][100/250]	LR: 0.1	Time 0.036 (0.040)	Data 0.019 (0.021)	Loss 0.2931 (0.2450)	Prec@1 89.000 (91.248)
Epoch: [38/80][200/250]	LR: 0.1	Time 0.047 (0.041)	Data 0.024 (0.022)	Loss 0.3446 (0.2540)	Prec@1 87.500 (91.020)
 * Training Prec@1 90.970
Test: [0/50]	Time 0.123 (0.123)	Loss 0.4162 (0.4162)	Prec@1 86.000 (86.000)
 * Testing Prec@1 86.290
Epoch: [39/80][0/250]	LR: 0.1	Time 0.146 (0.146)	Data 0.116 (0.116)	Loss 0.2374 (0.2374)	Prec@1 92.500 (92.500)
Epoch: [39/80][100/250]	LR: 0.1	Time 0.041 (0.042)	Data 0.021 (0.024)	Loss 0.3307 (0.2400)	Prec@1 92.000 (91.842)
Epoch: [39/80][200/250]	LR: 0.1	Time 0.032 (0.042)	Data 0.015 (0.023)	Loss 0.2426 (0.2457)	Prec@1 93.500 (91.515)
 * Training Prec@1 91.386
Test: [0/50]	Time 0.114 (0.114)	Loss 0.4451 (0.4451)	Prec@1 85.000 (85.000)
 * Testing Prec@1 86.110
Epoch: [40/80][0/250]	LR: 0.010000000000000002	Time 0.146 (0.146)	Data 0.117 (0.117)	Loss 0.2331 (0.2331)	Prec@1 91.500 (91.500)
Epoch: [40/80][100/250]	LR: 0.010000000000000002	Time 0.033 (0.043)	Data 0.015 (0.024)	Loss 0.1814 (0.1890)	Prec@1 94.000 (93.663)
Epoch: [40/80][200/250]	LR: 0.010000000000000002	Time 0.055 (0.043)	Data 0.036 (0.025)	Loss 0.1379 (0.1743)	Prec@1 94.500 (94.102)
 * Training Prec@1 94.170
Test: [0/50]	Time 0.126 (0.126)	Loss 0.3039 (0.3039)	Prec@1 91.500 (91.500)
 * Testing Prec@1 89.850
Epoch: [41/80][0/250]	LR: 0.010000000000000002	Time 0.121 (0.121)	Data 0.094 (0.094)	Loss 0.1498 (0.1498)	Prec@1 96.500 (96.500)
Epoch: [41/80][100/250]	LR: 0.010000000000000002	Time 0.045 (0.045)	Data 0.028 (0.027)	Loss 0.0870 (0.1463)	Prec@1 97.500 (94.955)
Epoch: [41/80][200/250]	LR: 0.010000000000000002	Time 0.047 (0.043)	Data 0.027 (0.025)	Loss 0.0873 (0.1428)	Prec@1 96.500 (95.129)
 * Training Prec@1 95.128
Test: [0/50]	Time 0.094 (0.094)	Loss 0.3172 (0.3172)	Prec@1 90.500 (90.500)
 * Testing Prec@1 90.010
Epoch: [42/80][0/250]	LR: 0.010000000000000002	Time 0.161 (0.161)	Data 0.122 (0.122)	Loss 0.1116 (0.1116)	Prec@1 96.000 (96.000)
Epoch: [42/80][100/250]	LR: 0.010000000000000002	Time 0.035 (0.047)	Data 0.020 (0.028)	Loss 0.1611 (0.1376)	Prec@1 93.000 (95.381)
Epoch: [42/80][200/250]	LR: 0.010000000000000002	Time 0.058 (0.044)	Data 0.030 (0.026)	Loss 0.1214 (0.1376)	Prec@1 96.000 (95.316)
 * Training Prec@1 95.316
Test: [0/50]	Time 0.116 (0.116)	Loss 0.3120 (0.3120)	Prec@1 90.500 (90.500)
 * Testing Prec@1 90.150
Epoch: [43/80][0/250]	LR: 0.010000000000000002	Time 0.132 (0.132)	Data 0.102 (0.102)	Loss 0.1844 (0.1844)	Prec@1 96.000 (96.000)
Epoch: [43/80][100/250]	LR: 0.010000000000000002	Time 0.048 (0.048)	Data 0.030 (0.029)	Loss 0.0794 (0.1266)	Prec@1 98.000 (95.767)
Epoch: [43/80][200/250]	LR: 0.010000000000000002	Time 0.039 (0.046)	Data 0.023 (0.028)	Loss 0.1735 (0.1288)	Prec@1 93.000 (95.709)
 * Training Prec@1 95.676
Test: [0/50]	Time 0.151 (0.151)	Loss 0.2896 (0.2896)	Prec@1 90.500 (90.500)
 * Testing Prec@1 90.090
Epoch: [44/80][0/250]	LR: 0.010000000000000002	Time 0.196 (0.196)	Data 0.162 (0.162)	Loss 0.1175 (0.1175)	Prec@1 97.500 (97.500)
Epoch: [44/80][100/250]	LR: 0.010000000000000002	Time 0.044 (0.039)	Data 0.018 (0.020)	Loss 0.1259 (0.1200)	Prec@1 96.000 (96.050)
Epoch: [44/80][200/250]	LR: 0.010000000000000002	Time 0.042 (0.039)	Data 0.023 (0.021)	Loss 0.1659 (0.1237)	Prec@1 95.000 (95.796)
 * Training Prec@1 95.822
Test: [0/50]	Time 0.132 (0.132)	Loss 0.3135 (0.3135)	Prec@1 89.500 (89.500)
 * Testing Prec@1 90.030
Epoch: [45/80][0/250]	LR: 0.010000000000000002	Time 0.184 (0.184)	Data 0.158 (0.158)	Loss 0.1279 (0.1279)	Prec@1 96.000 (96.000)
Epoch: [45/80][100/250]	LR: 0.010000000000000002	Time 0.036 (0.046)	Data 0.016 (0.028)	Loss 0.0887 (0.1232)	Prec@1 97.000 (95.772)
Epoch: [45/80][200/250]	LR: 0.010000000000000002	Time 0.042 (0.043)	Data 0.024 (0.024)	Loss 0.1043 (0.1231)	Prec@1 96.500 (95.721)
 * Training Prec@1 95.758
Test: [0/50]	Time 0.104 (0.104)	Loss 0.3016 (0.3016)	Prec@1 90.500 (90.500)
 * Testing Prec@1 90.280
Epoch: [46/80][0/250]	LR: 0.010000000000000002	Time 0.138 (0.138)	Data 0.105 (0.105)	Loss 0.0641 (0.0641)	Prec@1 98.000 (98.000)
Epoch: [46/80][100/250]	LR: 0.010000000000000002	Time 0.049 (0.046)	Data 0.031 (0.028)	Loss 0.0923 (0.1177)	Prec@1 96.000 (95.886)
Epoch: [46/80][200/250]	LR: 0.010000000000000002	Time 0.048 (0.045)	Data 0.027 (0.028)	Loss 0.0688 (0.1172)	Prec@1 99.500 (95.970)
 * Training Prec@1 95.978
Test: [0/50]	Time 0.149 (0.149)	Loss 0.3048 (0.3048)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.140
Epoch: [47/80][0/250]	LR: 0.010000000000000002	Time 0.126 (0.126)	Data 0.089 (0.089)	Loss 0.0921 (0.0921)	Prec@1 96.500 (96.500)
Epoch: [47/80][100/250]	LR: 0.010000000000000002	Time 0.052 (0.047)	Data 0.030 (0.028)	Loss 0.1079 (0.1081)	Prec@1 95.000 (96.208)
Epoch: [47/80][200/250]	LR: 0.010000000000000002	Time 0.036 (0.046)	Data 0.021 (0.027)	Loss 0.1424 (0.1118)	Prec@1 95.000 (96.097)
 * Training Prec@1 96.112
Test: [0/50]	Time 0.155 (0.155)	Loss 0.3216 (0.3216)	Prec@1 89.500 (89.500)
 * Testing Prec@1 90.080
Epoch: [48/80][0/250]	LR: 0.010000000000000002	Time 0.114 (0.114)	Data 0.087 (0.087)	Loss 0.0851 (0.0851)	Prec@1 96.500 (96.500)
Epoch: [48/80][100/250]	LR: 0.010000000000000002	Time 0.042 (0.045)	Data 0.028 (0.029)	Loss 0.1519 (0.1105)	Prec@1 93.500 (96.312)
Epoch: [48/80][200/250]	LR: 0.010000000000000002	Time 0.038 (0.044)	Data 0.022 (0.028)	Loss 0.0818 (0.1119)	Prec@1 97.500 (96.174)
 * Training Prec@1 96.202
Test: [0/50]	Time 0.104 (0.104)	Loss 0.3389 (0.3389)	Prec@1 89.000 (89.000)
 * Testing Prec@1 90.220
Epoch: [49/80][0/250]	LR: 0.010000000000000002	Time 0.142 (0.142)	Data 0.111 (0.111)	Loss 0.1023 (0.1023)	Prec@1 97.000 (97.000)
Epoch: [49/80][100/250]	LR: 0.010000000000000002	Time 0.044 (0.048)	Data 0.027 (0.030)	Loss 0.1420 (0.1067)	Prec@1 96.000 (96.406)
Epoch: [49/80][200/250]	LR: 0.010000000000000002	Time 0.060 (0.048)	Data 0.042 (0.030)	Loss 0.1120 (0.1040)	Prec@1 93.000 (96.430)
 * Training Prec@1 96.318
Test: [0/50]	Time 0.159 (0.159)	Loss 0.3374 (0.3374)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.200
Epoch: [50/80][0/250]	LR: 0.010000000000000002	Time 0.116 (0.116)	Data 0.094 (0.094)	Loss 0.1498 (0.1498)	Prec@1 95.500 (95.500)
Epoch: [50/80][100/250]	LR: 0.010000000000000002	Time 0.038 (0.045)	Data 0.022 (0.028)	Loss 0.1872 (0.1029)	Prec@1 94.500 (96.416)
Epoch: [50/80][200/250]	LR: 0.010000000000000002	Time 0.044 (0.043)	Data 0.027 (0.026)	Loss 0.0778 (0.1037)	Prec@1 96.500 (96.425)
 * Training Prec@1 96.414
Test: [0/50]	Time 0.109 (0.109)	Loss 0.3102 (0.3102)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.140
Epoch: [51/80][0/250]	LR: 0.010000000000000002	Time 0.184 (0.184)	Data 0.162 (0.162)	Loss 0.1044 (0.1044)	Prec@1 95.500 (95.500)
Epoch: [51/80][100/250]	LR: 0.010000000000000002	Time 0.052 (0.044)	Data 0.032 (0.027)	Loss 0.0941 (0.0990)	Prec@1 96.500 (96.743)
Epoch: [51/80][200/250]	LR: 0.010000000000000002	Time 0.036 (0.043)	Data 0.020 (0.026)	Loss 0.1070 (0.0986)	Prec@1 97.000 (96.756)
 * Training Prec@1 96.578
Test: [0/50]	Time 0.106 (0.106)	Loss 0.3178 (0.3178)	Prec@1 88.500 (88.500)
 * Testing Prec@1 89.950
Epoch: [52/80][0/250]	LR: 0.010000000000000002	Time 0.145 (0.145)	Data 0.114 (0.114)	Loss 0.1052 (0.1052)	Prec@1 96.000 (96.000)
Epoch: [52/80][100/250]	LR: 0.010000000000000002	Time 0.040 (0.045)	Data 0.023 (0.027)	Loss 0.0728 (0.1013)	Prec@1 97.000 (96.490)
Epoch: [52/80][200/250]	LR: 0.010000000000000002	Time 0.052 (0.044)	Data 0.032 (0.027)	Loss 0.0934 (0.1009)	Prec@1 97.000 (96.507)
 * Training Prec@1 96.470
Test: [0/50]	Time 0.163 (0.163)	Loss 0.3277 (0.3277)	Prec@1 90.000 (90.000)
 * Testing Prec@1 90.210
Epoch: [53/80][0/250]	LR: 0.010000000000000002	Time 0.142 (0.142)	Data 0.113 (0.113)	Loss 0.1233 (0.1233)	Prec@1 95.500 (95.500)
Epoch: [53/80][100/250]	LR: 0.010000000000000002	Time 0.041 (0.043)	Data 0.026 (0.026)	Loss 0.1295 (0.1013)	Prec@1 94.000 (96.441)
Epoch: [53/80][200/250]	LR: 0.010000000000000002	Time 0.040 (0.041)	Data 0.026 (0.025)	Loss 0.0932 (0.1001)	Prec@1 96.000 (96.517)
 * Training Prec@1 96.586
Test: [0/50]	Time 0.079 (0.079)	Loss 0.3279 (0.3279)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.200
Epoch: [54/80][0/250]	LR: 0.010000000000000002	Time 0.141 (0.141)	Data 0.112 (0.112)	Loss 0.1020 (0.1020)	Prec@1 95.000 (95.000)
Epoch: [54/80][100/250]	LR: 0.010000000000000002	Time 0.042 (0.043)	Data 0.024 (0.026)	Loss 0.1412 (0.0951)	Prec@1 94.000 (96.703)
Epoch: [54/80][200/250]	LR: 0.010000000000000002	Time 0.039 (0.042)	Data 0.024 (0.025)	Loss 0.1408 (0.0959)	Prec@1 94.000 (96.692)
 * Training Prec@1 96.704
Test: [0/50]	Time 0.128 (0.128)	Loss 0.3486 (0.3486)	Prec@1 91.500 (91.500)
 * Testing Prec@1 89.780
Epoch: [55/80][0/250]	LR: 0.010000000000000002	Time 0.152 (0.152)	Data 0.122 (0.122)	Loss 0.1475 (0.1475)	Prec@1 96.500 (96.500)
Epoch: [55/80][100/250]	LR: 0.010000000000000002	Time 0.043 (0.045)	Data 0.028 (0.029)	Loss 0.0506 (0.0929)	Prec@1 98.500 (96.782)
Epoch: [55/80][200/250]	LR: 0.010000000000000002	Time 0.040 (0.042)	Data 0.021 (0.026)	Loss 0.1077 (0.0943)	Prec@1 96.000 (96.701)
 * Training Prec@1 96.694
Test: [0/50]	Time 0.110 (0.110)	Loss 0.3403 (0.3403)	Prec@1 92.500 (92.500)
 * Testing Prec@1 89.990
Epoch: [56/80][0/250]	LR: 0.010000000000000002	Time 0.164 (0.164)	Data 0.134 (0.134)	Loss 0.0733 (0.0733)	Prec@1 98.000 (98.000)
Epoch: [56/80][100/250]	LR: 0.010000000000000002	Time 0.047 (0.040)	Data 0.027 (0.024)	Loss 0.1217 (0.0921)	Prec@1 96.500 (96.901)
Epoch: [56/80][200/250]	LR: 0.010000000000000002	Time 0.038 (0.041)	Data 0.023 (0.025)	Loss 0.0712 (0.0924)	Prec@1 97.500 (96.856)
 * Training Prec@1 96.854
Test: [0/50]	Time 0.159 (0.159)	Loss 0.3356 (0.3356)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.140
Epoch: [57/80][0/250]	LR: 0.010000000000000002	Time 0.140 (0.140)	Data 0.110 (0.110)	Loss 0.0485 (0.0485)	Prec@1 98.000 (98.000)
Epoch: [57/80][100/250]	LR: 0.010000000000000002	Time 0.039 (0.049)	Data 0.023 (0.030)	Loss 0.0957 (0.0922)	Prec@1 97.000 (96.950)
Epoch: [57/80][200/250]	LR: 0.010000000000000002	Time 0.035 (0.043)	Data 0.021 (0.026)	Loss 0.1300 (0.0930)	Prec@1 96.500 (96.896)
 * Training Prec@1 96.862
Test: [0/50]	Time 0.157 (0.157)	Loss 0.3204 (0.3204)	Prec@1 91.500 (91.500)
 * Testing Prec@1 90.130
Epoch: [58/80][0/250]	LR: 0.010000000000000002	Time 0.180 (0.180)	Data 0.147 (0.147)	Loss 0.0930 (0.0930)	Prec@1 98.500 (98.500)
Epoch: [58/80][100/250]	LR: 0.010000000000000002	Time 0.042 (0.044)	Data 0.025 (0.028)	Loss 0.0766 (0.0866)	Prec@1 97.500 (97.064)
Epoch: [58/80][200/250]	LR: 0.010000000000000002	Time 0.036 (0.043)	Data 0.019 (0.026)	Loss 0.0897 (0.0892)	Prec@1 97.000 (96.925)
 * Training Prec@1 96.958
Test: [0/50]	Time 0.162 (0.162)	Loss 0.3122 (0.3122)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.310
Epoch: [59/80][0/250]	LR: 0.010000000000000002	Time 0.117 (0.117)	Data 0.092 (0.092)	Loss 0.1160 (0.1160)	Prec@1 96.500 (96.500)
Epoch: [59/80][100/250]	LR: 0.010000000000000002	Time 0.035 (0.044)	Data 0.018 (0.026)	Loss 0.0960 (0.0875)	Prec@1 96.500 (96.931)
Epoch: [59/80][200/250]	LR: 0.010000000000000002	Time 0.042 (0.041)	Data 0.024 (0.023)	Loss 0.1298 (0.0878)	Prec@1 96.000 (96.993)
 * Training Prec@1 96.996
Test: [0/50]	Time 0.157 (0.157)	Loss 0.3177 (0.3177)	Prec@1 91.500 (91.500)
 * Testing Prec@1 90.040
Epoch: [60/80][0/250]	LR: 0.0010000000000000002	Time 0.122 (0.122)	Data 0.087 (0.087)	Loss 0.1248 (0.1248)	Prec@1 95.000 (95.000)
Epoch: [60/80][100/250]	LR: 0.0010000000000000002	Time 0.043 (0.040)	Data 0.023 (0.023)	Loss 0.1003 (0.0797)	Prec@1 95.500 (97.292)
Epoch: [60/80][200/250]	LR: 0.0010000000000000002	Time 0.040 (0.041)	Data 0.023 (0.024)	Loss 0.1141 (0.0795)	Prec@1 97.000 (97.366)
 * Training Prec@1 97.390
Test: [0/50]	Time 0.141 (0.141)	Loss 0.3047 (0.3047)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.340
Epoch: [61/80][0/250]	LR: 0.0010000000000000002	Time 0.110 (0.110)	Data 0.079 (0.079)	Loss 0.0938 (0.0938)	Prec@1 96.000 (96.000)
Epoch: [61/80][100/250]	LR: 0.0010000000000000002	Time 0.035 (0.037)	Data 0.021 (0.021)	Loss 0.0987 (0.0787)	Prec@1 95.500 (97.287)
Epoch: [61/80][200/250]	LR: 0.0010000000000000002	Time 0.035 (0.038)	Data 0.022 (0.022)	Loss 0.1240 (0.0787)	Prec@1 95.500 (97.226)
 * Training Prec@1 97.286
Test: [0/50]	Time 0.110 (0.110)	Loss 0.3211 (0.3211)	Prec@1 93.000 (93.000)
 * Testing Prec@1 90.320
Epoch: [62/80][0/250]	LR: 0.0010000000000000002	Time 0.154 (0.154)	Data 0.126 (0.126)	Loss 0.1199 (0.1199)	Prec@1 96.000 (96.000)
Epoch: [62/80][100/250]	LR: 0.0010000000000000002	Time 0.036 (0.043)	Data 0.019 (0.026)	Loss 0.0432 (0.0800)	Prec@1 99.000 (97.262)
Epoch: [62/80][200/250]	LR: 0.0010000000000000002	Time 0.038 (0.041)	Data 0.025 (0.024)	Loss 0.0813 (0.0769)	Prec@1 97.500 (97.351)
 * Training Prec@1 97.426
Test: [0/50]	Time 0.143 (0.143)	Loss 0.3186 (0.3186)	Prec@1 92.500 (92.500)
 * Testing Prec@1 90.240
Epoch: [63/80][0/250]	LR: 0.0010000000000000002	Time 0.142 (0.142)	Data 0.112 (0.112)	Loss 0.0774 (0.0774)	Prec@1 97.000 (97.000)
Epoch: [63/80][100/250]	LR: 0.0010000000000000002	Time 0.044 (0.043)	Data 0.028 (0.026)	Loss 0.0549 (0.0764)	Prec@1 99.000 (97.510)
Epoch: [63/80][200/250]	LR: 0.0010000000000000002	Time 0.035 (0.042)	Data 0.021 (0.025)	Loss 0.1031 (0.0775)	Prec@1 97.000 (97.403)
 * Training Prec@1 97.410
Test: [0/50]	Time 0.166 (0.166)	Loss 0.3273 (0.3273)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.180
Epoch: [64/80][0/250]	LR: 0.0010000000000000002	Time 0.141 (0.141)	Data 0.107 (0.107)	Loss 0.0584 (0.0584)	Prec@1 98.000 (98.000)
Epoch: [64/80][100/250]	LR: 0.0010000000000000002	Time 0.033 (0.039)	Data 0.018 (0.023)	Loss 0.0890 (0.0744)	Prec@1 97.000 (97.470)
Epoch: [64/80][200/250]	LR: 0.0010000000000000002	Time 0.034 (0.039)	Data 0.022 (0.023)	Loss 0.0450 (0.0745)	Prec@1 99.000 (97.545)
 * Training Prec@1 97.524
Test: [0/50]	Time 0.171 (0.171)	Loss 0.3282 (0.3282)	Prec@1 92.500 (92.500)
 * Testing Prec@1 90.390
Epoch: [65/80][0/250]	LR: 0.0010000000000000002	Time 0.137 (0.137)	Data 0.108 (0.108)	Loss 0.0925 (0.0925)	Prec@1 96.500 (96.500)
Epoch: [65/80][100/250]	LR: 0.0010000000000000002	Time 0.053 (0.041)	Data 0.034 (0.024)	Loss 0.1002 (0.0782)	Prec@1 96.500 (97.347)
Epoch: [65/80][200/250]	LR: 0.0010000000000000002	Time 0.044 (0.041)	Data 0.026 (0.025)	Loss 0.1139 (0.0755)	Prec@1 96.000 (97.463)
 * Training Prec@1 97.518
Test: [0/50]	Time 0.168 (0.168)	Loss 0.3212 (0.3212)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.260
Epoch: [66/80][0/250]	LR: 0.0010000000000000002	Time 0.158 (0.158)	Data 0.131 (0.131)	Loss 0.1017 (0.1017)	Prec@1 97.000 (97.000)
Epoch: [66/80][100/250]	LR: 0.0010000000000000002	Time 0.037 (0.043)	Data 0.023 (0.026)	Loss 0.1430 (0.0752)	Prec@1 95.500 (97.525)
Epoch: [66/80][200/250]	LR: 0.0010000000000000002	Time 0.033 (0.041)	Data 0.019 (0.025)	Loss 0.0574 (0.0738)	Prec@1 98.000 (97.572)
 * Training Prec@1 97.518
Test: [0/50]	Time 0.109 (0.109)	Loss 0.3236 (0.3236)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.250
Epoch: [67/80][0/250]	LR: 0.0010000000000000002	Time 0.112 (0.112)	Data 0.083 (0.083)	Loss 0.0751 (0.0751)	Prec@1 96.500 (96.500)
Epoch: [67/80][100/250]	LR: 0.0010000000000000002	Time 0.041 (0.043)	Data 0.023 (0.025)	Loss 0.0760 (0.0733)	Prec@1 96.000 (97.540)
Epoch: [67/80][200/250]	LR: 0.0010000000000000002	Time 0.037 (0.041)	Data 0.022 (0.024)	Loss 0.0580 (0.0737)	Prec@1 98.000 (97.555)
 * Training Prec@1 97.576
Test: [0/50]	Time 0.148 (0.148)	Loss 0.3181 (0.3181)	Prec@1 92.500 (92.500)
 * Testing Prec@1 90.250
Epoch: [68/80][0/250]	LR: 0.0010000000000000002	Time 0.188 (0.188)	Data 0.157 (0.157)	Loss 0.0281 (0.0281)	Prec@1 99.500 (99.500)
Epoch: [68/80][100/250]	LR: 0.0010000000000000002	Time 0.036 (0.044)	Data 0.022 (0.026)	Loss 0.0290 (0.0717)	Prec@1 99.500 (97.644)
Epoch: [68/80][200/250]	LR: 0.0010000000000000002	Time 0.051 (0.041)	Data 0.030 (0.024)	Loss 0.0551 (0.0732)	Prec@1 99.500 (97.552)
 * Training Prec@1 97.522
Test: [0/50]	Time 0.160 (0.160)	Loss 0.3148 (0.3148)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.260
Epoch: [69/80][0/250]	LR: 0.0010000000000000002	Time 0.203 (0.203)	Data 0.171 (0.171)	Loss 0.0633 (0.0633)	Prec@1 97.500 (97.500)
Epoch: [69/80][100/250]	LR: 0.0010000000000000002	Time 0.037 (0.047)	Data 0.024 (0.029)	Loss 0.0763 (0.0781)	Prec@1 97.500 (97.421)
Epoch: [69/80][200/250]	LR: 0.0010000000000000002	Time 0.036 (0.043)	Data 0.019 (0.026)	Loss 0.0607 (0.0744)	Prec@1 97.500 (97.507)
 * Training Prec@1 97.510
Test: [0/50]	Time 0.156 (0.156)	Loss 0.3173 (0.3173)	Prec@1 92.500 (92.500)
 * Testing Prec@1 90.270
Epoch: [70/80][0/250]	LR: 0.0010000000000000002	Time 0.119 (0.119)	Data 0.089 (0.089)	Loss 0.0781 (0.0781)	Prec@1 97.000 (97.000)
Epoch: [70/80][100/250]	LR: 0.0010000000000000002	Time 0.038 (0.046)	Data 0.023 (0.029)	Loss 0.0511 (0.0717)	Prec@1 98.500 (97.649)
Epoch: [70/80][200/250]	LR: 0.0010000000000000002	Time 0.049 (0.045)	Data 0.030 (0.027)	Loss 0.0929 (0.0732)	Prec@1 96.500 (97.545)
 * Training Prec@1 97.574
Test: [0/50]	Time 0.110 (0.110)	Loss 0.3178 (0.3178)	Prec@1 92.500 (92.500)
 * Testing Prec@1 90.210
Epoch: [71/80][0/250]	LR: 0.0010000000000000002	Time 0.127 (0.127)	Data 0.090 (0.090)	Loss 0.0518 (0.0518)	Prec@1 99.000 (99.000)
Epoch: [71/80][100/250]	LR: 0.0010000000000000002	Time 0.046 (0.045)	Data 0.028 (0.027)	Loss 0.0622 (0.0748)	Prec@1 97.500 (97.559)
Epoch: [71/80][200/250]	LR: 0.0010000000000000002	Time 0.044 (0.044)	Data 0.022 (0.026)	Loss 0.0892 (0.0729)	Prec@1 97.500 (97.577)
 * Training Prec@1 97.604
Test: [0/50]	Time 0.139 (0.139)	Loss 0.3217 (0.3217)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.200
Epoch: [72/80][0/250]	LR: 0.0010000000000000002	Time 0.160 (0.160)	Data 0.128 (0.128)	Loss 0.1026 (0.1026)	Prec@1 98.000 (98.000)
Epoch: [72/80][100/250]	LR: 0.0010000000000000002	Time 0.034 (0.040)	Data 0.020 (0.024)	Loss 0.0672 (0.0764)	Prec@1 97.500 (97.450)
Epoch: [72/80][200/250]	LR: 0.0010000000000000002	Time 0.034 (0.038)	Data 0.017 (0.023)	Loss 0.1295 (0.0746)	Prec@1 96.000 (97.547)
 * Training Prec@1 97.562
Test: [0/50]	Time 0.129 (0.129)	Loss 0.3201 (0.3201)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.170
Epoch: [73/80][0/250]	LR: 0.0010000000000000002	Time 0.156 (0.156)	Data 0.119 (0.119)	Loss 0.0722 (0.0722)	Prec@1 97.000 (97.000)
Epoch: [73/80][100/250]	LR: 0.0010000000000000002	Time 0.036 (0.039)	Data 0.025 (0.024)	Loss 0.0609 (0.0708)	Prec@1 97.500 (97.629)
Epoch: [73/80][200/250]	LR: 0.0010000000000000002	Time 0.038 (0.039)	Data 0.025 (0.024)	Loss 0.0648 (0.0717)	Prec@1 98.000 (97.614)
 * Training Prec@1 97.558
Test: [0/50]	Time 0.147 (0.147)	Loss 0.3291 (0.3291)	Prec@1 91.500 (91.500)
 * Testing Prec@1 90.210
Epoch: [74/80][0/250]	LR: 0.0010000000000000002	Time 0.157 (0.157)	Data 0.126 (0.126)	Loss 0.0530 (0.0530)	Prec@1 98.000 (98.000)
Epoch: [74/80][100/250]	LR: 0.0010000000000000002	Time 0.037 (0.044)	Data 0.022 (0.027)	Loss 0.0977 (0.0726)	Prec@1 97.500 (97.609)
Epoch: [74/80][200/250]	LR: 0.0010000000000000002	Time 0.036 (0.043)	Data 0.024 (0.027)	Loss 0.0695 (0.0720)	Prec@1 97.000 (97.597)
 * Training Prec@1 97.622
Test: [0/50]	Time 0.136 (0.136)	Loss 0.3262 (0.3262)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.300
Epoch: [75/80][0/250]	LR: 0.0010000000000000002	Time 0.191 (0.191)	Data 0.163 (0.163)	Loss 0.0460 (0.0460)	Prec@1 99.500 (99.500)
Epoch: [75/80][100/250]	LR: 0.0010000000000000002	Time 0.045 (0.046)	Data 0.026 (0.028)	Loss 0.0589 (0.0693)	Prec@1 97.500 (97.832)
Epoch: [75/80][200/250]	LR: 0.0010000000000000002	Time 0.038 (0.044)	Data 0.024 (0.027)	Loss 0.1185 (0.0705)	Prec@1 95.500 (97.726)
 * Training Prec@1 97.732
Test: [0/50]	Time 0.155 (0.155)	Loss 0.3351 (0.3351)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.220
Epoch: [76/80][0/250]	LR: 0.0010000000000000002	Time 0.132 (0.132)	Data 0.102 (0.102)	Loss 0.1240 (0.1240)	Prec@1 95.500 (95.500)
Epoch: [76/80][100/250]	LR: 0.0010000000000000002	Time 0.035 (0.042)	Data 0.017 (0.025)	Loss 0.0819 (0.0723)	Prec@1 97.000 (97.624)
Epoch: [76/80][200/250]	LR: 0.0010000000000000002	Time 0.046 (0.040)	Data 0.030 (0.024)	Loss 0.0421 (0.0706)	Prec@1 99.000 (97.672)
 * Training Prec@1 97.690
Test: [0/50]	Time 0.144 (0.144)	Loss 0.3242 (0.3242)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.240
Epoch: [77/80][0/250]	LR: 0.0010000000000000002	Time 0.159 (0.159)	Data 0.131 (0.131)	Loss 0.0853 (0.0853)	Prec@1 98.000 (98.000)
Epoch: [77/80][100/250]	LR: 0.0010000000000000002	Time 0.057 (0.043)	Data 0.036 (0.025)	Loss 0.0358 (0.0681)	Prec@1 99.000 (97.782)
Epoch: [77/80][200/250]	LR: 0.0010000000000000002	Time 0.037 (0.043)	Data 0.021 (0.026)	Loss 0.1074 (0.0726)	Prec@1 96.000 (97.617)
 * Training Prec@1 97.674
Test: [0/50]	Time 0.117 (0.117)	Loss 0.3222 (0.3222)	Prec@1 92.000 (92.000)
 * Testing Prec@1 90.250
Epoch: [78/80][0/250]	LR: 0.0010000000000000002	Time 0.127 (0.127)	Data 0.098 (0.098)	Loss 0.0950 (0.0950)	Prec@1 97.000 (97.000)
Epoch: [78/80][100/250]	LR: 0.0010000000000000002	Time 0.065 (0.041)	Data 0.042 (0.026)	Loss 0.0778 (0.0715)	Prec@1 98.000 (97.535)
Epoch: [78/80][200/250]	LR: 0.0010000000000000002	Time 0.039 (0.042)	Data 0.025 (0.026)	Loss 0.0709 (0.0717)	Prec@1 97.500 (97.649)
 * Training Prec@1 97.640
Test: [0/50]	Time 0.155 (0.155)	Loss 0.3330 (0.3330)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.300
Epoch: [79/80][0/250]	LR: 0.0010000000000000002	Time 0.101 (0.101)	Data 0.073 (0.073)	Loss 0.1099 (0.1099)	Prec@1 95.500 (95.500)
Epoch: [79/80][100/250]	LR: 0.0010000000000000002	Time 0.035 (0.043)	Data 0.023 (0.027)	Loss 0.0499 (0.0697)	Prec@1 97.500 (97.743)
Epoch: [79/80][200/250]	LR: 0.0010000000000000002	Time 0.041 (0.042)	Data 0.022 (0.026)	Loss 0.0913 (0.0712)	Prec@1 96.500 (97.682)
 * Training Prec@1 97.680
Test: [0/50]	Time 0.150 (0.150)	Loss 0.3376 (0.3376)	Prec@1 91.000 (91.000)
 * Testing Prec@1 90.170
